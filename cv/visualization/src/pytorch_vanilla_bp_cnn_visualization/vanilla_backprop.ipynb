{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as T  # 对PIL.Image进行变换\n",
    "from torchsummary import summary\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenet_val(num=None):\n",
    "    \"\"\"Load a handful of validation images from ImageNet.\n",
    "    Inputs:\n",
    "    - num: Number of images to load (max of 25)\n",
    "    Returns:\n",
    "    - X: numpy array with shape [num, 224, 224, 3]\n",
    "    - y: numpy array of integer image labels, shape [num]\n",
    "    - class_names: dict mapping integer label to class name\n",
    "    \"\"\"\n",
    "    imagenet_fn = './imagenet_val_25.npz'\n",
    "    f = np.load(imagenet_fn,  allow_pickle=True)   # Allow loading pickled object arrays stored in npy files\n",
    "    X = f['X']\n",
    "    y = f['y']\n",
    "    class_names = f['label_map'].item()\n",
    "    if num is not None:\n",
    "        X = X[:num]\n",
    "        y = y[:num]\n",
    "    return X, y, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "def preprocess(img, size=224):\n",
    "    \"\"\"\n",
    "    因为transform的function都是针对PIL.Image.Image 所以在这里img需要的输入为PIL.Image.Image类型,从Image.fromarray(x)得到\n",
    "    \"\"\"\n",
    "    transform = T.Compose([                \n",
    "        T.Resize(size),                    \n",
    "        T.ToTensor(),                                                                \n",
    "        T.Normalize(mean = mean,std = std),      \n",
    "        T.Lambda(lambda x: x[None]),                                               \n",
    "    ])\n",
    "    return transform(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "            Conv2d-4         [-1, 64, 224, 224]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 224, 224]             128\n",
      "              ReLU-6         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-7         [-1, 64, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]          73,856\n",
      "       BatchNorm2d-9        [-1, 128, 112, 112]             256\n",
      "             ReLU-10        [-1, 128, 112, 112]               0\n",
      "           Conv2d-11        [-1, 128, 112, 112]         147,584\n",
      "      BatchNorm2d-12        [-1, 128, 112, 112]             256\n",
      "             ReLU-13        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-14          [-1, 128, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         295,168\n",
      "      BatchNorm2d-16          [-1, 256, 56, 56]             512\n",
      "             ReLU-17          [-1, 256, 56, 56]               0\n",
      "           Conv2d-18          [-1, 256, 56, 56]         590,080\n",
      "      BatchNorm2d-19          [-1, 256, 56, 56]             512\n",
      "             ReLU-20          [-1, 256, 56, 56]               0\n",
      "           Conv2d-21          [-1, 256, 56, 56]         590,080\n",
      "      BatchNorm2d-22          [-1, 256, 56, 56]             512\n",
      "             ReLU-23          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-24          [-1, 256, 28, 28]               0\n",
      "           Conv2d-25          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-26          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-27          [-1, 512, 28, 28]               0\n",
      "           Conv2d-28          [-1, 512, 28, 28]       2,359,808\n",
      "      BatchNorm2d-29          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-30          [-1, 512, 28, 28]               0\n",
      "           Conv2d-31          [-1, 512, 28, 28]       2,359,808\n",
      "      BatchNorm2d-32          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-33          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-34          [-1, 512, 14, 14]               0\n",
      "           Conv2d-35          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-36          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-37          [-1, 512, 14, 14]               0\n",
      "           Conv2d-38          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-39          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-40          [-1, 512, 14, 14]               0\n",
      "           Conv2d-41          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-42          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-43          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-44            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-45            [-1, 512, 7, 7]               0\n",
      "           Linear-46                 [-1, 4096]     102,764,544\n",
      "             ReLU-47                 [-1, 4096]               0\n",
      "          Dropout-48                 [-1, 4096]               0\n",
      "           Linear-49                 [-1, 4096]      16,781,312\n",
      "             ReLU-50                 [-1, 4096]               0\n",
      "          Dropout-51                 [-1, 4096]               0\n",
      "           Linear-52                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,365,992\n",
      "Trainable params: 0\n",
      "Non-trainable params: 138,365,992\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 322.14\n",
      "Params size (MB): 527.82\n",
      "Estimated Total Size (MB): 850.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# read pre-train model\n",
    "model = torchvision.models.vgg16_bn(pretrained=True)  # last layer linear(fc) , pretrained: If True, returns a model pre-trained on ImageNet\n",
    "# model = torchvision.models.alexnet(pretrained=True)   # last layer linear(fc) \n",
    "# model = torchvision.models.squeezenet1_1(pretrained=True)   # AdaptiveAvgPool2d \n",
    "\n",
    "\n",
    "# seeting the model parameters as can not trained status\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # make sure the model is 'test' mode\n",
    "    model.eval()\n",
    "    X_var = Variable(X,requires_grad=True)\n",
    "    y_var = Variable(y)                # make the tensor into Variable\n",
    "    salicency = None\n",
    "    \n",
    "    # forward pass\n",
    "    score = model(X_var)\n",
    "\n",
    "    # Get the correct class computed scores. 得到label对应的score\n",
    "    # view 类似于numpy的reshape, 如果不确定要多少行,但是确定列数, 可以设定为(-1,1)这意味着确定列数为1\n",
    "    # torch.gather, 沿着axis=1的方向, 取index为y_var.view(-1, 1)的数\n",
    "    scores = score.gather(1, y_var.view(-1, 1)).squeeze()  \n",
    "\n",
    "    # Backward pass, need to supply initial gradients of same tensor shape as scores.\n",
    "    scores.backward(torch.FloatTensor([1.0,1.0,1.0,1.0,1.0]))\n",
    "    \n",
    "    # Get gradient for image.\n",
    "    saliency = X_var.grad.data\n",
    "    \n",
    "    # Convert from 3d to 1d.\n",
    "    saliency = saliency.abs()\n",
    "    saliency, i = torch.max(saliency,dim=1)\n",
    "#     saliency = saliency.squeeze() \n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './imagenet_val_25.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-76cbcab0658b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_imagenet_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mshow_saliency_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fd3f10ff60af>\u001b[0m in \u001b[0;36mload_imagenet_val\u001b[0;34m(num)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m     10\u001b[0m     \u001b[0mimagenet_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./imagenet_val_25.npz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagenet_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Allow loading pickled object arrays stored in npy files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Certis/liveness-detection/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './imagenet_val_25.npz'"
     ]
    }
   ],
   "source": [
    "def show_saliency_maps(X,y):\n",
    "    \n",
    "    # torch.cat: Concatenates the given sequence of seq tensors in the given dimension\n",
    "    # dim:  the dimension over which the tensors are concatenated\n",
    "    X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0).cuda()\n",
    "    y_tensor = torch.LongTensor(y).cuda()\n",
    "    \n",
    "    # Compute saliency maps for images in X\n",
    "    saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n",
    "    \n",
    "    # Convert the saliency map from Torch Tensor to numpy array and show images\n",
    "    # and saliency maps together.\n",
    "    saliency = saliency.cpu().numpy()\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    for i in range(N):\n",
    "        plt.subplot(2, N, i + 1)\n",
    "        plt.imshow(X[i])\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y[i]])\n",
    "        plt.subplot(2, N, N + i + 1)\n",
    "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(12, 5)\n",
    "        \n",
    "X, y, class_names = load_imagenet_val(num=5)       \n",
    "show_saliency_maps(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liveness-detection",
   "language": "python",
   "name": "liveness-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
